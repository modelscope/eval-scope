# input raw data
question_file: llmuses/registry/data/arena/question_mini.jsonl

# candidate models to be battled
answers_gen:
    gpt-3.5-turbo:
        enable: true           # enable or disable this model
        ref: llmuses.tasks.task_eval_gpt35_completion:TaskEvalGpt35Completion
        args:
            max_tokens: 1024
            temperature: 0.2
        output_file: llmuses/registry/data/arena/answers/answer_gpt35_dummy.jsonl
    vicuna-13b:
        enable: false
        ref: null
        args:
            max_tokens: 512
            temperature: 0.2
        output_file: llmuses/registry/data/arena/answers/answer_vicuna-13b.jsonl
    llama-7b:
        enable: false
        ref: null
        args:
            max_tokens: 512
            temperature: 0.2
        output_file: llmuses/registry/data/arena/answers/answer_llama-7b.jsonl

# model of auto-reviewer
reviews_gen:
    enable: true
    reviewer:
        ref: llmuses.evaluator.auto_reviewer_gpt4:AutoReviewerGpt4
        args:
            max_tokens: 1024
            temperature: 0.2
            # options: pairwise_all, pairwise_baseline, single, default is pairwise_all
            mode: pairwise_all
            # position bias mitigation strategy, options: swap_position, randomize_order, None. default is None
            position_bias_mitigation: None
            # completion parser config, default is lmsys_parser
            fn_completion_parser: lmsys_parser
    # target answers list to be reviewed, could be replaced by your own path: /path/to/answers.jsonl
    target_answers: [llmuses/registry/data/arena/answers/answer_gpt35.jsonl,
                     llmuses/registry/data/arena/answers/answer_gpt35_dummy.jsonl]
    # prompt templates for auto reviewer(GPT-4)
    prompt_file: llmuses/registry/data/arena/prompt_template/lmsys.jsonl
    # output file of auto reviewer
    review_file: llmuses/registry/data/arena/reviews/review_gpt4.jsonl

# rating results
rating_gen:
    enable: true
    metrics: ['elo']
    # elo rating report file
    report_file: llmuses/registry/data/arena/reports/elo_rating.csv
